\documentclass{llncs}
%\documentclass[a4paper]{article}

%\usepackage{fullpage}
\usepackage{setspace}
\usepackage{url}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{subfig}

%\onehalfspacing

%\newtheorem{definition}{Definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem{example}{Example}
\newtheorem{query}{Query}

\newcommand{\nop}[1]{}

\setlength{\tabcolsep}{2pt}

\newcommand{\myremark}[2]{{\textbf{\marginpar{$\parallel$}(* \textit{#1:} #2 *)}}}

\title{Mapping Microblog Posts to Encyclopedia Articles}
\author{Uta L\"{o}sch \and David M\"{u}ller \and Andreas Harth}
\institute{
	Karlsruhe Institute of Technology (KIT), D-76131 Karlsruhe, Germany\\ 
	\email{uta.loesch@kit.edu},\\
	\email{david.mueller@student.kit.edu},\\
	\email{harth@kit.edu}
}
\begin{document}

\maketitle

\begin{abstract}
To facilitate understanding the meaning of a hash tag in the context of Twitter, we have developed a system which annotates hash tags based on Twitter search results with encyclopedia entities that best describe the hash tag, thus offering means of quickly grasping the meaning of the hash tag and at the same time providing starting points for further exploration of the hash tags' context. In an evaluation we show that the encyclopedia entities used for annotation represent the content in a suitable manner and that the annotations remain stable over time, i.e. when executing the same search at different times, the same encyclopedia entities are returned.
\end{abstract}

Keywords: Twitter, Wikipedia, DBpedia, RDF

\nop{
http://tagdef.com/ has a list of popular tags
}

\section{Introduction}

Twitter\footnote{\url{http://twitter.com}} is a micro-blogging service that has become very influential over the last years. The idea of micro-blogs is the same as that of blogs, except that the message length is restricted, in the case of Twitter the restriction is 140 characters. Twitter has a total of about 190 million of users who produce 65 million Twitter messages a day. Thus, Twitter represents a huge data source on the web.

On Twitter it is possible to search for messages containing a specific hash tag. This search returns a fixed number of most recent messages containing the tag. However, it is hard to grasp the context of the results and to get further information on the topic that was searched for. As each single Twitter message is very short and contains little information, it is necessary to parse the whole set in order to get an overview of the context(s) in which the search term is used. 

To facilitate this putting into context of the search results, we propose to annotate search results with a set of entities which reflect the content of the result feeds. These entities will not only help to understand the search terms, but also serve as a starting point for refining the search or searching for further information related to the search result. 

Furthermore, this annotation will help to understand hash tags. Hash tags are frequently used to associate messages with a specific topic, place, person or event on Twitter. For example, messages talking about what is going on at the European Semantic Web Conference 2011 will frequently be tagged with $\#eswc2011$. If a user encounters this hash tag and does not understand it, he can search for it on Twitter. However, as discussed before, to understand a search result is difficult and time-consuming. Using our approach the search will not only return messages using the tag but also a set of entities which have been detected in the search result. These entities will help the user to gain an understanding in which context the hash tag is used and what it may mean.

Finding entities matching a query, especially a query for a given hash tag is difficult due to synonyms and homonyms in the set of hash tags that are frequently used. For example, both the hash tags $\#ka$ and $\#karlsruhe$ are frequently used when talking about topics which are related to the city Karlsruhe. The tag $\#ka$ is however also used in the context of the car type Ford Ka. Thus, when searching for $\#ka$ on Twitter, messages relating to Karlsruhe will be found as well as messages related to Ford Ka. An approach which aims at finding relevant entities for a given search will have to be able to deal with these problems. 


In this paper, we present a system which automatically annotates a Twitter search result with Wikipedia entities. While it would be more interesting to annotate each single Twitter message with relevant entities, Twitter messages are too short to contain much relevant information which could be used as input for the annotation tool. Thus, we chose to generate annotations for the whole search result. 

The motivation for choosing Wikipedia as entity source was that it offers a wide coverage of topics. Furthermore, automatic tools for annotating text with Wikipedia entities are readily available (see \cite{key:wikifier}). Note however that a direct mapping of hash tags to entities is not possible in many cases: e.g. there is no entity in dbpedia which describes the European Semantic Web Conference 2011, thus the tag $\#eswc2011$ can not mapped directly.

Annotations should be stable over time, unless the topics discussed in the Twitter messages containing the search term change. In an evaluation of our approach we have tried to analyze the stability of the result.

Our contributions thus are:
\begin{itemize}
	\item an approach for finding entities which are related to a search on Twitter
	\item an implementation of our approach
	\item an evaluation of the stability and relevance of the entities found with our approach.
\end{itemize}

The rest of this paper is organized as follows: in Section~\ref{sect:relWork} we discuss related work, in Section~\ref{sect:method} we present our approach, the results of our evaluation are presented in Section~\ref{sect:eval}, before we conclude in Section~\ref{sect:conclusion}.

\section{Related Work}
\label{sect:relWork}

Bringing Semantic Web technologies and Semantic Web technologies together, has been proposed several times.

The SemanticTweet\footnote{\url{http://semantictweet.com}} service is a tool which allows for the generation of a FOAF file describing one's network of followers and friends on Twitter. Thus, an automatic way of mapping the Twitter social network to semantic data is made available.

Passant et al. \cite{key:smob} have proposed a data model for making Twitter data available on the Semantic Web. They propose a data model which allows for the association of URIs with users, microblogs and microposts. To this end, SIOC and FOAF vocabularies are used and extended. Specifically, the new concepts \emph{Microblog} and \emph{MicroBlogPost} are introduced. Additionally, they propose the use of so-called semantic hash tags. The idea is to use URIs as hash tags (e.g. \emph{\#geo:Paris\_France}). It becomes thus possible to link microposts to entities in the Linked Open Data cloud via these semantic hash tags. 
In contrast to this approach we do not require users to change their behavior by requiring them to use another type of hash tags. Instead, our method finds Linked Open Data entities which are related to the query automatically.

Softic et al \cite{key:softic} have developed a framework and system for mining data from social networks. They have instantiated their system to analyze data from Twitter. In their system, they collect data from one or several users. The system by Passant et al. \cite{key:smob} is used for transforming twitter data to semantic data. The triplified data can then be analysed using SPARQL. They propose that the result data could be enriched using entities from other Linked Open Data vocabularies like dbpedia\footnote{\url{http://dbpedia.org}} or GeoNames\footnote{\url{http://www.geonames.org}}, but they do not provide a method for achieving this.

Relating semantic concepts with twitter messages has been attempted by various authors:
Stankovic et al. \cite{key:stanko} propose to use the Zemanta\footnote{\url{http://developer.zemanta.com/}} keyword extraction API to detect topics of single Twitter messages. 
% from stakovic et al.
%In order to make tweets more searchable and make them useful for the described
%scenario one would need to have represent them in a structured format and associate
%those tweets with the topics that refer to. As a basic approach we process the text of
%the tweets using Zemanta16 keyword extraction API. This approach gives DBPedia
%concepts related to a tweet, but is useful only on a limited number of tweets directly
%mentioning a particular topic. In the example data set we produced from ESWC2010,
%tweet topics are represented using dc:subject property.
Instead of mining single messages which provide little signal to the extraction algorithm we use an aggregation of messages (as proposed by Wagner \cite{key:clauwa}) as input to the extraction component.

Wagner \cite{key:clauwa} proposes to use aggregations of social awareness streams for knowledge extraction. As such, our approach is closely related.

Kinsella et al. \cite{key:kinsella} use hyperlinks in posts to derive a richer representation of those posts. We too use external data to augment the set of Twitter messages supplied to the Wikifier system.

The challenge task in the WePS-3 Online Reputation Management Task\cite{key:clef} requires systems to detect mentions of company names in Twitter messages.
The corpus contains evaluation results solicited via Mechanical Turk. In contrast to the WePS-3 task, we associate generic hash tags to Wikipedia/DBpedia concepts.






\section{Method Overview}
\label{sect:method}

The goal of our method is to, given a search query $q$ as input, provide the user with an RDF\footnote{\url{http://www.w3.org/RDF/}} document describing the search result $R$, i.e. the most recent messages that match the query, the authors of these messages and the most relevant entities $E$ for this result.

The architecture of our proposed system is shown in Figure~\ref{fig:arch}. Upon receipt of a query four steps are performed. In the following we will illustrate these steps using the example of a search for the hash tag $\#ka$, a hashtag which is frequently used to describe posts related to the city of Karlsruhe, Germany.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.7\linewidth]{architecture}
  \caption{System Architecture}
  \label{fig:arch}
\end{figure}

\begin{enumerate}
	\item In a first step, the most recent search results for the query are
	obtained.\newline In our example, calling the Wrapper with query $\#ka$ will
	return a document containing all most recent Twitter posts whose content matched the search string $\#ka$ and their meta information.

An example for a single retrieved post is the following: \begin{sloppypar}
''\texttt{Just
returned from a wonderful holiday in \#ka.
\url{http://www.karlsruhe.de/stadt/tourismus.en}
@KarlsruheTweets}'',\end{sloppypar} posted by Twitter user
\texttt{sample\_user\_12345}; geo coordinates and the date the message has been posted are furthermore known.
  
  \item In a second (optional) step hyperlinks posted in messages of the fetched
  feed are followed, content of the referenced website is fetched and processed by the Wrapper. The idea is to acquire additional content which is used as additional signal when searching for entities. We have thought of other methods for enriching the result, like searching for other messages posted by users which have posted messages in the result set. However, these alternative methods are more likely to introduce noise in the signal which is used for obtaining relevant entities.
  
In our example, the message contains a link to
\url{http://www.karlsruhe.de/stadt/tourismus.en}, the official homepage of the
tourist office of the city of Karlsruhe. Dereferencing this URL and grabbing the
text found at the link gives us the following text:
\begin{sloppypar}''\texttt{Karlsruhe, where paths converge - a legendary city in the sunny south-west of Germany, famed for its fan-shaped historic street plan, where margraves reigned in times past, and German joie de vivre reigns today.}''
\end{sloppypar}	
	\item In a third step entities matching the search result are retrieved. The
	content of the Twitter messages that matched the search query and, if available, the content of referenced websites is merged to a single input string and used as input for the annotation. The result of this step is a list of matching articles of the English Wikipedia.

In our example, the contents of all posts related to the search term '\#ka' are
merged with the retrieved external website content. Thus, the post obtained from
step 1 would be matched with the website content obtained in step 2 into a
single input stream: \begin{sloppypar}"\texttt{Just returned from a wonderful
holiday in \#ka. \url{http://www.karlsruhe.de/stadt/tourismus.en}
@KarlsruheTweets. Karlsruhe, where paths converge - a legendary city in the
sunny south-west of Germany, famed for its fan-shaped historic street plan, where margraves reigned in times past, and German joie de vivre reigns today.}"\end{sloppypar}This string is appended to the Twitter content and the external content of all other posts in the result.

In the example, we would expect to find entities which are related to Karlsruhe, like Karlsruhe, Germany or the Karlsruhe\_Institute\_of\_Technology.

	\item	Finally, an RDF document is generated representing the search results and the annotations.  
	
	Continuing our example, the output RDF document should contain the following
	triples to reference the DBPedia mappings:
    \begin{enumerate}
      \item \texttt{\#ka rdfs:seeAlso http://dbpedia.org/resource/Karlsruhe.}
      \item \texttt{\#ka rdfs:seeAlso http://dbpedia.org/resource/Germany.}
      \item \texttt{\#ka rdfs:seeAlso\newline
      http://dbpedia.org/resource/Karlsruhe\_Institute\_of\_Technology.}
    \end{enumerate}

	The Twitter posts matching the search term \#ka are likewise all described in
	the output document. Here, the description for the initially given post
	abbreviated \texttt{m} results in the following
	triples:
	\begin{enumerate}
      \item \texttt{twitter:m foaf:page\newline
      ``http://twitter.com/sample\_user\_12345/statuses/1234567890.''}
      \item \texttt{twitter:m dc:date ``2011-02-23T12:00:00Z''.}
      \item \texttt{twitter:m geo:lat ``49.010239''.}
      \item \texttt{twitter:m geo:long ``8.411879''.}
      \item \texttt{twitter:m foaf:maker twitter:sample\_user\_12345}
      \item \texttt{twitter:m dc:description ``Just returned from a
      wonderful\newline holiday in
      \#ka.\url{http://www.karlsruhe.de/stadt/tourismus.en}\newline @KarlsruheTweets''.}
    \end{enumerate}
\end{enumerate}

\section{Implementation}

In the Twitter Search Wrapper\footnote{Available at \url{http://km.aifb.kit.edu/services/twittersearchwrap/}} we provide an implementation of our approach.

The implementation is based on a set of existing components: Twitter search results are obtained by using the Twitter search API\footnote{\url{http://dev.twitter.com/doc/get/search}}. The result is a feed containing the 100 most recently published Twitter messages, which are publicly visible and match the query, and their authors. The messages themselves are described by their content, publishing date, authors
and optionally a geographic location.

The optional step of dereferencing URLs which are posted in messages contained
in the result set can be triggered using the option \texttt{extern=true} in our
system. To keep the input size for the next steps manageable, we do not use the
whole content of the referenced sites but only the first 50 words. The content
retrieved from external websites is beautified for our purpose removing
HTML-tags and scripting elements using the CyberNeko Java HTML Parser
Library\footnote{\url{http://sourceforge.net/projects/nekohtml/}}.

The Wikifier \cite{key:wikifier} is used for obtaining content annotations. This tool takes a text as input and returns a set of Wikipedia entities which are relevant for the analyzed text.

The system generates and outputs an RDF document containing a
semantic description of the retrieved messages matching the search query for the
given query and references to the DBpedia articles that have been returned by the Wikifier. It describes
the result by means of the popular vocabularies FOAF \cite{key:foaf}, Dublin Core\footnote{\url{http://dublincore.org/documents/dcmi-terms/}}, the Basic Geo Vocabulary \cite{key:geo}. The annotations are added to the search result using \texttt{rdf:seeAlso} links to DBpedia entities \cite{key:dbpedia} which are obtained by a direct transformation of the Wikipedia entities' URLs. DBpedia has been chosen as these entities allow for a more coherent presentation of the content in the context of RDF.

The example twitter message given in the previous section is represented in
the output of the Twitter Search Wrapper as follows:\newline
\linebreak
\texttt{<rdf:Description
rdf:about="http://km.aifb.kit.edu/services/\newline
twitterwrap/statuses/show/1234567890\#id">\newline
	<foaf:page
	rdf:resource="http://twitter.com/sample\_user\_12345/\newline
	statuses/1234567890"/>\newline
	<dc:date>2011-02-23T12:00:00Z</dc:date>\newline <geo:lat>49.010239</geo:lat>\newline 
	<geo:long>8.411879</geo:long>\newline
	<foaf:maker
	rdf:resource="http://km.aifb.kit.edu/services/\newline
	twitterwrap/users/show?screen\_name=sample\_user\_12345\#id"/>\newline <dc:description>\newline
	Just returned from a wonderful holiday in \#ka.
	\url{http://www.karlsruhe.de/stadt/tourismus.en} @KarlsruheTweets\newline
	</dc:description>\newline
	</rdf:Description>\newline}
	\linebreak
Furthermore the mappings to DBpedia articles returned by the Wikifier
are represented in the output document. In the Twitter Search Wrapper the
representation is done the following way, again using the introductory
example from the previous section:\newline
\linebreak
	\texttt{<rdf:Description rdf:about=""/>}\newline
	\texttt{<rdfs:seeAlso rdf:resource="http://dbpedia.org/resource/Karlsruhe"/>}\newline
	\texttt{<rdfs:seeAlso rdf:resource="http://dbpedia.org/resource/Germany"/>}\newline
	\texttt{<rdfs:seeAlso rdf:resource="http://dbpedia.org/resource/\newline
	Karlsruhe\_Institute\_of\_Technology"/>}\newline
	\texttt{</rdf:Description>}\newline

\section{Experiments and Evaluation}
\label{sect:eval}

We have evaluated our approach to assess whether the entities which are found as related to a specific hash tag are useful and stable over time.

\subsection{Setup}

We have conducted the evaluation on two sets of queries: the first set are the trending topics worldwide and in Germany on Twitter retrieved on February 24th 17:20h (GMT) -- these topics are listed in Table \ref{tbl:terms}. Trending topics are chosen as they cover a variety of recently talked about topics which due to its recent nature may be of high interest for further processing. 

\begin{table}[ht*]
\centering
\begin{tabular}{ l }
Trending Topics Worldwide  \\
\hline
\#goodlucknickj  \\
\#smalljoys \\
\#thingsthedevilinvented \\
Adam Wainwright  \\
CatchPetrovaFire \\
Gadafi  \\
Libia \\
Precious Minaj \\
Tommy John  \\
\end{tabular}
\begin{tabular}{l}
Trending Topics Germany \\
\hline
\#gaddafi \\
\#guttenberg  \\
\#libya  \\
Doktorarbeit \\
Gef\"{u}hl  \\
Lion  \\
MBP \\
Thunderbolt \\
Vorratsdatenspeicherung \\
\end{tabular}
\caption{Search terms}\label{tbl:terms}
\end{table}


The second query set consists of hash tags that are frequently used on Twitter. To obtain these, we have sampled the Twitter message feed using Twitter's Streaming API for one minute twice an hour from 2011-03-03 to 2011-03-06. The top-50 hashtags dervied from the aggregate sample are listed in Table \ref{tbl:hashtags}, count is the number of times we encountered them during the collection period. 



\begin{table}[ht*]
\centering
\begin{tabular}{ l|l|l|l|l|l|l|l|l }
Hashtag & Count & $N_{ext}$ & $E(aut)$ & $Var(aut)$ & $E(ts)$ & $Var(ts)$ & $E(ts_{ext})$ & $Var(ts_{ext})$\\
\hline
ff &     460 & 48 & 2.2936 & 0.7014 & 0.1268 & 0.0681 & 0.2037 & 0.0281 \\
np &     172 & 48 & 0.4737 & 0.0287 & 0.0655 & 0.0196 & 0.0717 & 0.0195\\
nowplaying & 144 & 17 & 0.4514 & 0.0251 & 0.0565 & 0.0198 & 0.0458 & 0.0212 \\
winning &     134 & 48 & 0.8236 & 0.2365 & 0.1101 & 0.0666 & 0.1816 & 0.0272 \\
fb & 105 & 1 & 1.1705 & 1.1826 & 0.1083 & 0.0933 & - & - \\
rt & 83 & 48 & 1.7980 & 3.7309 & 0.0979 & 0.0763 & 0.1487 & 0.0267 \\
mentionke & 61 & 48 & 7.5459 & 195.6829 & 0.1406 & 0.0921 & 0.2215 & 0.0408 \\
blackpeoplemovies & 57 & 3 & 90.6195 & 2007.5551 & 0.3327 & 0.0606 & 0.0349 & 0.0244\\
tigerblood & 56 & 48 & 3.4029 & 4.8740 & 0.1497 & 0.0692 & 0.2330 & 0.0278 \\
thataintwinning & 53 & 48 & 16.8075 & 236.8694 & 0.0768 & 0.0435 & 0.1668 & 0.0387 \\
fail & 53 & 48 & 2.3356 & 2.0669 & 0.1927 & 0.1517 & 0.1073 & 0.0460 \\
teamfollowback & 52 & 48 & 0.9269 & 0.4871 & 0.1912 & 0.0932 & 0.1813 & 0.0245 \\
iftwitterwashighschool & 51 & 48 & 114.8342 & 1771.4564 & 0.5113 & 0.0405 & 0.5144 & 0.0352 \\
bieberfact & 51 & 48 & 11.0126 & 63.0455 & 0.0587 & 0.0290 & 0.0765 & 0.0270 \\
news & 46 & 0 & 1.8696 & 0.1943 & 0.1024 & 0.0210 & - & -  \\
jobs & 45 & 0 & 1.3866 & 0.4198 & 0.1006 & 0.0192 & - & - \\
libya & 44 & 1 & 16.0309 & 117.2024 & 0.2701 & 0.0427 & - & - \\
followmejp & 44 & 0	& 1.5398 & 0.1255 & 0.0688 & 0.0410 & - & -\\
partiu & 41 & 48 & 7.5809 & 84.0531 & 0.6042 & 0.2391 & 0.5313 & 0.2438 \\
sougofollow & 39 & 0 & 2.1010 & 0.1820 & 0.0720 & 0.0427 & - & - \\
damnitstrue & 39 & 48 & 5.7507 & 16.5725 & 0.1805 & 0.1387 & 0.2177 & 0.1579 \\
tilltheworldends & 37 & 48 & 85.4526 & 1366.8588 & 0.5682 & 0.0709 & 0.5116 & 0.0324 \\
soalcinta & 35 & 48 & 8.4143 & 103.7817 & 0.3451 & 0.1987 & 0.3120 & 0.2014 \\
nicovideo & 35 & 0 & 51.0720 & 5521.3529 & 0.3604 & 0.1893 & - & - \\
quran & 33 & 48 & 45.8290 & 90.4157 & 0.4271 & 0.0501 & 0.4188 & 0.0387 \\
%\end{tabular}
%\begin{tabular}{ l|l|l|l|l|l|l|l|l }
%Hashtag & Count  \\
%\hline
nw & 32 & 48 & 2.6054 & 2.1094 & 0.0672 & 0.0231 & 0.0901 & 0.0418 \\
quote & 31 & 48 & 4.3007 & 0.5706 & 0.1074 & 0.0197 & 0.1012 & 0.0212 \\
shoutout & 30 & 48 & 2.6030 & 3.3256 & 0.2347 & 0.0987 & 0.2526 & 0.0488 \\
jfb & 30 & 48 & 23.9911 & 1453.3113 & 0.2483 & 0.1654 & 0.2158 & 0.1379 \\
carnaval & 27 & 48 & 34.2574 & 1100.9818 & 0.3229 & 0.2134 & 0.2578 & 0.1646 \\
tfb & 26 & 48 & 2.5994 & 0.8777 & 0.2292 & 0.1766 & 0.1760 & 0.0319 \\
followme & 26 & 0 & 3.2194 & 0.4650 & 0.1209 & 0.0952 &	- &	- \\
followfriday & 25 & 48 & 13.6749 & 20.0701 & 0.1910 & 0.1152 & 0.2060 & 0.0676 \\
follow & 24 & 48 & 3.0945 & 1.9622 & 0.2916 & 0.1927 & 0.2046 & 0.0305 \\
egypt & 23 & 16 & 3.1850 & 3.2745 & 0.1357 & 0.0239 & 0.0704 & 0.0263 \\
retweet & 22 & 48 & 4.6991 & 13.4434 & 0.1570 & 0.1232 & 0.1353 & 0.0306 \\
fact & 22 & 48 & 5.5613 & 12.0766 & 0.0915 & 0.0762 & 0.0783 & 0.0612 \\
win & 20 & 0 & 4.2949 & 5.1856 & 0.0347 & 0.0221 & - & - \\
tweetmyjobs & 20 & 0 & 35.5697 & 1590.6997 & 0.3244 & 0.0621 & - & - \\
music & 20 & 1 & 4.4665 & 4.6392 & 0.0833 & 0.0196 & - & - \\
in & 20 & 3	& 8.2367 & 6.8944 & 0.0649 & 0.0236 & 0.0251 & 0.0206 \\
imagine & 20 & 48	& 8.3378 & 30.4317 & 0.2465 & 0.1759 & 0.1813 & 0.1375 \\
nowwatching & 19 & 48 & 7.9110 & 8.6833 & 0.0672 & 0.0214 & 0.1293 & 0.0211 \\
cumannanyaindonesia & 19 & 48 & 77.9680 & 2739.7782 & 0.3901 & 0.1885 & 0.4323 & 0.2033 \\
twexit & 18 & 48 & 12.5672 & 255.8178 & 0.3125 & 0.2148 & 0.2417 & 0.1783 \\
lcv & 18 & 48 & 28.6309 & 477.9193 & 0.4236 & 0.2395 & 0.2813 & 0.1969 \\
fui & 18 & 48 & 11.5515 & 165.3850 & 0.4896 & 0.2447 & 0.4635 & 0.2448 \\
bot &      18 & 1 & 13.3533 &	8.2469 & 0.2708 & 0.1975 & - & -  \\
nemuritsuzuketeshinu & 17 & 48 & 314.1120 & 25053.5657 & 1.0000 & 0.0000 & 1.0000 & 0.0000 \\
fato & 17 & 48 & 17.6223 & 569.5627 & 0.1875 & 0.1523 & 0.2353 & 0.1304 \\
\end{tabular}
\caption{Popular hashtags sampled during a three-day period.}\label{tbl:hashtags}
\end{table}


The goal of our evaluation was to examine the stability of the entity annotations over time. To achieve this, we have repeatedly executed the same set of queries and analysed the returned data. 

In principle, two methods for data acquisition were identified: \begin{definition}[Data Acquisition Methods]
\begin{itemize}
\item {\bf Temporal Sliding Window}\newline The query is executed and the n
most recent results are retrieved every t minutes. The Temporal Sliding Window
method may lead to duplicate messages in the retrieved documents, but needs to
handle much less data and allows for an analysis over larger time frames.
\item {\bf $k$-item Sliding Window}\newline  The k-item Sliding Window methods
retrieves all messages that match the query exactly one time. Each time k new messages are
available matching the given query a document containing those k messages
is retrieved. This method can lead to high data volume and needs constant
monitoring of recently posted messages matching the search term.
\end{itemize}
\end{definition}

The disadvantage of the first method is that it is neither possible to guarantee that all messages matching the search are collected, nor is it possible to avoid overlaps between the search results. However, while the second method seems to overcome these problems, it does not: Using the $k$-item sliding window method, it is not clear when the next search result should be fetched: even when frequently querying for new messages, if updates are frequent it is likely to lack messages in the result set.

For both data set queries were executed twice per hour using the Temporal Sliding Window method for data acquisition in a 24 hour time frame. Data for the trending topics was collected on 2011-02-24, for the frequent hash tags data was collected on 2011-03-07. 

\subsection{Measures}

For analysing the collected data, several measures were defined:

\begin{definition}[Evaluation Measures]
Given disjoint result sets $R_1,\ldots,R_n$ for a query $q$ obtained at times $t_1,\ldots,t_n$ and annotated with sets of entities $E_1,\ldots,E_n$, where each result set $R_i$ consists of twitter messages $r_i1,\ldots,r_iK$ with time stamps $t_i1,\ldots,t_iK$ we define:
\begin{itemize}
	\item The average update time within a result set $aut(R_i)$ for a query is the average time that lies between messages that match the query which produced the search result: $$aut(R_i)=\frac{\sum_{j=1}^{n}t_{i,j+1}-t_{i,j}}{K-1}$$
	A high $aut(R)$ means that new messages that match the query are not generated very often. If new messages matching the query are frequently published, then the $adt(R)$ is low.
	
	The mean $aut(q)$ and the variance $autv(q)$ of the $aut(R_i)$ can be used to describe update properties for the result sets that are generated for a given query. E.g. if for a given query $aut(q)$ is high and $autv(q)$ is low, then the rate at which messages are published is stable at a quite low level.
%	\item The global stability $gs$ of the results shows how stable the whole set of entity terms for a query is. It is defined as: $$gs(q)=\frac{|\bigcap_{i=1}^{n}E_i|}{|\bigcup_{i=1}^{n}E_i|}$$
%	A value of $gs(q)$ of $1$ means that the set of entities is the same at each point in time $t_i$. On the other hand, a value of $0$ means that at least two of the sets of entities, with which the results were annotated, are disjoint.
	\item The stability for a single term is defined as the fraction of result sets for a query $q$ in which it occurs: 
$$sts(q,t)=\frac{|\{i|t\in E_i\}|}{n}$$
	If the value of $sts$ is $1$ the term occurs in all of the result sets for the query, if it is close to $0$ it occurs in few result sets.
	\item The time stability $ts$ for a query $q$ at time $t_i$ is the fraction of
	entities which are unchanged between the result set at $t_{i-1}$ and $t_i$: $$ts(q,t_i)=\begin{cases} 1 & i=1 \\
														\frac{|E_{i-1}\cap E_i|}{|E_{i-1}\cup E_i|} & i>1\end{cases}$$
	A value of $1$ means that the two sets of entities are identical, a value of $0$ means they are disjoint.
	
	The variance of time stabilities $ts(q,t_i)$ over all $t_i$ helps to determine whether the rate at which the found entities change is rather stable or whether it is changing a lot over time.
\end{itemize}
\end{definition}

\subsection{Results}

An overview of the results we obtained for the frequently used hash tags is given in Table~\ref{tbl:hashtags}. In total, we have analysed 48 result sets for each of the query tags. However, in some cases, the analysis which includes the content of external web pages led to a timeout of the Wikifier. $N_{ext}$ gives the number of result sets which we could analyse when the external content is included. $E(aut)$ and $Var(aut)$ give the results for analysing the average update times of the result sets. A low $E(aut)$ means that updates are frequently posted, a high $E(aut)$ means that updates are rather infrequent. In the case of our analysis estimated update frequencies range from $0.45$ seconds to $314.11$ seconds, i.e. approximately $5$ minutes. In order to avoid any duplicates in the result sets, an average update time of less than $18$ seconds is needed. Note that this value is only an average and does not guarantee disjointness of the result sets, as high update frequencies in one result set may outweigh low update frequencies in other result sets. Thus, we have additionally analysed the variance of the $aut$s. A low variance means that the update intervals remain relatively stable over time, while a high variance indicates that messages using the hash tag were posted rather irregularly. In general, higher average update times are associated with a higher variance in the update times. In some cases however, the variance of the average update time is surprisingly low when the mean average update time is rather high: this is e.g. the case for the tags $\#bieberfact$, $\#quran$, $\#followfriday$. For these tags updates came at a relatively regular level.

We also report the average time stability over all time points as well as the variance for each hash tag. A high time stability indicates that more or less the same entities were used for annotation at all times. The high stability of $1$ was achieved for the tag $\#nemuritsuzuketeshinu$, because no entities could be associated with the results of this search. This can be explained as the tag is only used by Japanese twitter users in Japanese twitter messages. However, the Wikifier analyses the content of the results as being English text.
A similar problem was encountered when analysing the trending topics in Germany. We assumed that these topics would mostly be tweeted about in German and thus called the German version of the Wikifier when analysing the results. However, it turned out that although the topics were trending in Germany, the tags were also used in messages in other languages.

Surprisingly, the second highest stability value was reached with the tag $\#partiu$ which is mainly used in Spanish twitter messages. 
\myremark{Uta}{Liegt auch an nichtgefundenen Entitäten}

The third highest stability was achieved by the tag $\#tilltheworldends$ which refers to Britney Spears' newest single. Accordingly, the entities which were most frequently used for annotation are "Britney Spears" ($95.83\%$), "iTunes" ($68.75\%$), "itunes" ($62.5\%$), "Itunes" ($31.25\%$), "Madonna\_(entertainer)" ($16.6\%$). The values in brackets indicate the single term stabilities for each of the terms.
\myremark{Uta}{Weiß jemand, wie Madonna da reinkommt? Gibt es ne Erklärung für die unterschiedlichen Schreibweisen von iTunes? JA, liegt am Wikifier der keine normalisierten URIs zurückliefert}

Very low time stability was obtained for the tag $\#nowplaying$ which is used to tell which song one is currently listening to (a similarly low stability is obtained for the tag $\#np$ which is often used synonymously with $\#nowplaying$). As these messages contain all kinds of artist names and songs names. It is thus expected that the entities which are found vary between the result sets. In this case the most frequent entities were "iTunes" ($52.08\%$), "Michael Jackson" ($33.3\%$), "Lady Gaga" ($33.3\%$), "Lil Wayne" ($29.16\%$), "Katy Perry" ($22.92\%$).
%\myremark{Uta}{In der Evaluationstabelle sollte dann auf jeden Fall noch dabei stehen, wie viele Resultsets wir zu den einzelnen Queries haben}

The effect of considering external content in the annotation is beneficial in some cases, in others the effect was destabilizing. 
\myremark{Uta}{Meine Hypothese wäre, dass es in Fällen, wo auf sehr unterschiedliche Quellen verwiesen wird, destabilisierend wirkt, in Fällen wo auf die gleichen/ähnliche Seiten verwiesen wird, stabilisierend - habe aber keine Daten, um das zu überprüfen.}
\myremark{todo}{Hypothesis: high average update time leads to stable results}

\subsection{Discussion}

@@@highlight the remarkable outcomes
@@@what is did we learn from the results?


\section{Conclusion}
\label{sect:conclusion}

@@@ not possible to map one hashtag to a single dbpedia concept: \#ka may be used for Karlsruhe or Ford Ka

@@@ tricky to deal with different languages - wikifier (or any nlp technique) is language dependent, but the trending topics may be commented on in a variety of languages

@@@ how to select external data (parse text from HTML, which part of web page to select - boilerplate stuff might spoil the results (reference blog cleansing paper

\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}
